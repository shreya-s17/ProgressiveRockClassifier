{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NN-MFCCs.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"metadata":{"id":"PWgtAnM3pGk0","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import librosa\n","\n","from torch.autograd import Variable\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"K2lAHLKueK5s","colab_type":"text"},"cell_type":"markdown","source":["# Interface colab notebook with dataset in google drive"]},{"metadata":{"id":"4y_YdIwXroj4","colab_type":"code","outputId":"383d0b62-66d4-42d2-ab25-44785d0efcd2","executionInfo":{"status":"ok","timestamp":1554761906650,"user_tz":240,"elapsed":88690,"user":{"displayName":"Nick Kroeger","photoUrl":"","userId":"17654142378335071446"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"metadata":{"id":"7vfrRFPhseQJ","colab_type":"code","outputId":"47d78f21-0b64-4a35-b692-a9e51fe6176f","executionInfo":{"status":"ok","timestamp":1554761923213,"user_tz":240,"elapsed":13216,"user":{"displayName":"Nick Kroeger","photoUrl":"","userId":"17654142378335071446"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["#Check google drive directory \n","!ls \"/content/drive/My Drive/Colab Notebooks/\"\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":0,"outputs":[{"output_type":"stream","text":[" NN-MFCCs.ipynb   NonProg  'Progressive Rock Songs'\n"],"name":"stdout"}]},{"metadata":{"id":"IMHAhkMletFF","colab_type":"text"},"cell_type":"markdown","source":["# Get mp3 file names"]},{"metadata":{"id":"n8a_zKaw3kym","colab_type":"code","outputId":"424becc7-e65d-40ec-bc40-07f5a518051c","executionInfo":{"status":"ok","timestamp":1554761928493,"user_tz":240,"elapsed":4476,"user":{"displayName":"Nick Kroeger","photoUrl":"","userId":"17654142378335071446"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["from google.colab import drive\n","\n","import glob\n","\n","filePaths_NonProg = glob.glob(\"/content/drive/My Drive/Colab Notebooks/NonProg/***.mp3\")\n","filePaths_Prog = glob.glob(\"/content/drive/My Drive/Colab Notebooks/Progressive Rock Songs/***.mp3\")\n","print(len(filePaths_NonProg))\n","print(len(filePaths_Prog))\n","numberNonProg = len(filePaths_NonProg)\n","numberProg = len(filePaths_Prog)\n","#for i in range(len(filePaths_Prog)):\n","#  print(filePaths_Prog[i])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["302\n","73\n"],"name":"stdout"}]},{"metadata":{"id":"0ZREhRhueDOU","colab_type":"text"},"cell_type":"markdown","source":["# Extract Features - MFCCs and Covariance of MFCCs"]},{"metadata":{"id":"ua9BJmMm7iyz","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","numOfMFCCS = 20\n","\n","def getMFCCS(x, sr, nMFCCs):\n","    stft = np.abs(librosa.stft(x, n_fft=2048, hop_length=512))\n","    mel = librosa.feature.melspectrogram(sr=sr, S=stft**2)\n","    mfccs = librosa.feature.mfcc(S=librosa.power_to_db(mel), n_mfcc=20)\n","    meanMFCCS = np.mean(mfccs, axis=1)\n","\n","    cov = np.cov(mfccs)\n","    upperCovIndicies = np.triu_indices(nMFCCs)\n","    upperCov = cov[upperCovIndicies]\n","    return meanMFCCS, upperCov, mfccs\n","\n","def normalizeInputs(x):\n","    mean = x.mean()\n","    std = x.std()\n","    z = (x-mean)/std\n","    return z"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GVadxlKQe8Y_","colab_type":"text"},"cell_type":"markdown","source":["Grab features from files, store into matrix"]},{"metadata":{"id":"GWLHqiyI2cHV","colab_type":"code","outputId":"8d44a5fb-f5ce-464f-9101-4f01800d3999","executionInfo":{"status":"ok","timestamp":1554762284190,"user_tz":240,"elapsed":359230,"user":{"displayName":"Nick Kroeger","photoUrl":"","userId":"17654142378335071446"}},"colab":{"base_uri":"https://localhost:8080/","height":5151}},"cell_type":"code","source":["inputSize = 230\n","nonProgInputs = np.zeros((numberNonProg, inputSize))\n","for i, file in enumerate(filePaths_NonProg):\n","  print(i)\n","  x, sr = librosa.load(file, sr=None, mono=True, duration = 40)\n","  meanMFCC, covMFCC, mfccs = getMFCCS(x, sr, numOfMFCCS)\n","  meanMFCCNormalized = normalizeInputs(meanMFCC)\n","  covMFCCNormalized = normalizeInputs(covMFCC)\n","  \n","  inputs = np.concatenate((meanMFCCNormalized, covMFCCNormalized))\n","  inputs = np.expand_dims(inputs,axis=1).T\n","\n","  nonProgInputs[i,:] = inputs\n","  "],"execution_count":0,"outputs":[{"output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","37\n","38\n","39\n","40\n","41\n","42\n","43\n","44\n","45\n","46\n","47\n","48\n","49\n","50\n","51\n","52\n","53\n","54\n","55\n","56\n","57\n","58\n","59\n","60\n","61\n","62\n","63\n","64\n","65\n","66\n","67\n","68\n","69\n","70\n","71\n","72\n","73\n","74\n","75\n","76\n","77\n","78\n","79\n","80\n","81\n","82\n","83\n","84\n","85\n","86\n","87\n","88\n","89\n","90\n","91\n","92\n","93\n","94\n","95\n","96\n","97\n","98\n","99\n","100\n","101\n","102\n","103\n","104\n","105\n","106\n","107\n","108\n","109\n","110\n","111\n","112\n","113\n","114\n","115\n","116\n","117\n","118\n","119\n","120\n","121\n","122\n","123\n","124\n","125\n","126\n","127\n","128\n","129\n","130\n","131\n","132\n","133\n","134\n","135\n","136\n","137\n","138\n","139\n","140\n","141\n","142\n","143\n","144\n","145\n","146\n","147\n","148\n","149\n","150\n","151\n","152\n","153\n","154\n","155\n","156\n","157\n","158\n","159\n","160\n","161\n","162\n","163\n","164\n","165\n","166\n","167\n","168\n","169\n","170\n","171\n","172\n","173\n","174\n","175\n","176\n","177\n","178\n","179\n","180\n","181\n","182\n","183\n","184\n","185\n","186\n","187\n","188\n","189\n","190\n","191\n","192\n","193\n","194\n","195\n","196\n","197\n","198\n","199\n","200\n","201\n","202\n","203\n","204\n","205\n","206\n","207\n","208\n","209\n","210\n","211\n","212\n","213\n","214\n","215\n","216\n","217\n","218\n","219\n","220\n","221\n","222\n","223\n","224\n","225\n","226\n","227\n","228\n","229\n","230\n","231\n","232\n","233\n","234\n","235\n","236\n","237\n","238\n","239\n","240\n","241\n","242\n","243\n","244\n","245\n","246\n","247\n","248\n","249\n","250\n","251\n","252\n","253\n","254\n","255\n","256\n","257\n","258\n","259\n","260\n","261\n","262\n","263\n","264\n","265\n","266\n","267\n","268\n","269\n","270\n","271\n","272\n","273\n","274\n","275\n","276\n","277\n","278\n","279\n","280\n","281\n","282\n","283\n","284\n","285\n","286\n","287\n","288\n","289\n","290\n","291\n","292\n","293\n","294\n","295\n","296\n","297\n","298\n","299\n","300\n","301\n"],"name":"stdout"}]},{"metadata":{"id":"HzvgJtKM-_GD","colab_type":"code","outputId":"5535ed56-84e3-45c9-9856-bce645285b31","executionInfo":{"status":"ok","timestamp":1554762372825,"user_tz":240,"elapsed":446955,"user":{"displayName":"Nick Kroeger","photoUrl":"","userId":"17654142378335071446"}},"colab":{"base_uri":"https://localhost:8080/","height":1258}},"cell_type":"code","source":["progInputs = np.zeros((numberProg, inputSize))\n","for i, file in enumerate(filePaths_Prog):\n","  print(i)\n","  x, sr = librosa.load(file, sr=None, mono=True, duration = 40)\n","  meanMFCC, covMFCC, mfccs = getMFCCS(x, sr, numOfMFCCS)\n","  meanMFCCNormalized = normalizeInputs(meanMFCC)\n","  covMFCCNormalized = normalizeInputs(covMFCC)\n","  \n","  inputs = np.concatenate((meanMFCCNormalized, covMFCCNormalized))\n","  inputs = np.expand_dims(inputs,axis=1).T\n","\n","  progInputs[i,:] = inputs\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","37\n","38\n","39\n","40\n","41\n","42\n","43\n","44\n","45\n","46\n","47\n","48\n","49\n","50\n","51\n","52\n","53\n","54\n","55\n","56\n","57\n","58\n","59\n","60\n","61\n","62\n","63\n","64\n","65\n","66\n","67\n","68\n","69\n","70\n","71\n","72\n"],"name":"stdout"}]},{"metadata":{"id":"BA-FcJwH3iGc","colab_type":"code","outputId":"857d69bc-9ca3-4c96-99ef-95bda05167ee","executionInfo":{"status":"ok","timestamp":1554762372826,"user_tz":240,"elapsed":446504,"user":{"displayName":"Nick Kroeger","photoUrl":"","userId":"17654142378335071446"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["#Combine prog with non prog inputs\n","allInputs = np.concatenate((progInputs, nonProgInputs))\n","allInputs.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(375, 230)"]},"metadata":{"tags":[]},"execution_count":11}]},{"metadata":{"id":"jv3C0mYIfLnN","colab_type":"text"},"cell_type":"markdown","source":["# Create data labels"]},{"metadata":{"id":"5NTwr8pdEowe","colab_type":"code","colab":{}},"cell_type":"code","source":["#Create labels \n","yProg = np.ones((numberProg,1))\n","yNonProg = -np.ones((numberNonProg,1))\n","y = np.concatenate((yProg, yNonProg))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bQPsh-WTfZUd","colab_type":"text"},"cell_type":"markdown","source":["# Split dataset into training set and test set"]},{"metadata":{"id":"yPGSXvhhF5Ed","colab_type":"code","colab":{}},"cell_type":"code","source":["#splitting the training set\n","testSize = .2\n","randStateSplit = 42\n","inputTrain, inputTest, labelTrain, labelTest = train_test_split(allInputs, y, test_size=testSize, random_state = randStateSplit, shuffle = True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xeAw4j18fjWm","colab_type":"text"},"cell_type":"markdown","source":["# Convert our dataset into a pseudo-pytorch data loader format to feed into the Neural Network (can be improved)"]},{"metadata":{"id":"XV6RNjB-NMKa","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","numTrainingSamples = inputTrain.shape[0]\n","input_size = inputTrain.shape[1]\n","\n","batch_size = 25\n","numOfTrainingBatches = 12\n","\n","inputTrainPD = pd.DataFrame(np.concatenate((labelTrain, inputTrain),axis = 1))\n","\n","train_loader = []\n","labels = []\n","for g, temp_df in inputTrainPD.groupby(np.arange(numTrainingSamples) // batch_size):\n","  tempLabels = np.asarray(temp_df.iloc[:,0])\n","  tempLabels = torch.LongTensor(tempLabels)\n","  tempLabelsSqueezed = torch.squeeze(tempLabels)\n","  labels.append(tempLabelsSqueezed)\n","\n","  tempInputs = temp_df.iloc[:,1:].as_matrix() #convert to np array ... as_matrix returns np array\n","  torchSamples = []\n","  for i in range(batch_size):\n","    #import pdb; pdb.set_trace()\n","    tempTorchSample = torch.FloatTensor(tempInputs[i])\n","    torchSamples.append(torch.t(tempTorchSample.view(input_size,1)))\n","  #   import pdb; pdb.set_trace()\n","  stackedInputTensors = torch.stack(torchSamples)    \n","\n","  train_loader.append(stackedInputTensors.view(batch_size,input_size))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KEjq4R9rfuRx","colab_type":"text"},"cell_type":"markdown","source":["# Neural Network Architecture"]},{"metadata":{"id":"bGR6z-25fxxV","colab_type":"code","colab":{}},"cell_type":"code","source":["hidden_size1 = 200\n","hidden_size2 = 50\n","num_classes = 1\n","learning_rate = .001\n","# Fully connected neural network with one hidden layer\n","class NeuralNet(nn.Module):\n","    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes):\n","        super(NeuralNet, self).__init__()\n","        self.fc1 = nn.Linear(input_size, hidden_size1) \n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(hidden_size1, hidden_size2) \n","        self.relu = nn.ReLU()\n","        self.fc3 = nn.Linear(hidden_size2, num_classes)  \n","    \n","    def forward(self, x):\n","        out = self.fc1(x)\n","        out = self.relu(out)\n","        out = self.fc2(out)\n","        out = self.relu(out)\n","        out = self.fc3(out)\n","        return out\n","\n","net = NeuralNet(input_size, hidden_size1, hidden_size2, num_classes).to(device)\n","\n","# Loss and optimizer\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)  \n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rcZK3Ee1gCmY","colab_type":"text"},"cell_type":"markdown","source":["# Train model"]},{"metadata":{"id":"QJrmU7ADIf7C","colab_type":"code","outputId":"c8848893-2d8e-44f8-ad69-e6e6a12b9431","executionInfo":{"status":"ok","timestamp":1554767277956,"user_tz":240,"elapsed":1258,"user":{"displayName":"Nick Kroeger","photoUrl":"","userId":"17654142378335071446"}},"colab":{"base_uri":"https://localhost:8080/","height":9401}},"cell_type":"code","source":["num_epochs = 100\n","# Train the model\n","total_step = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i in range(numOfTrainingBatches):  \n","        # Move tensors to the configured device\n","        input_x = Variable(train_loader[i])\n","        label_x = Variable(labels[i])\n","        \n","        # Forward pass\n","        outputs = net(input_x.float())\n","        outputs = torch.squeeze(outputs)\n","        \n","        #import pdb; pdb.set_trace()\n","        loss = criterion(outputs, label_x.float())\n","        if loss <= 0.008:\n","          break\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        #if (i+1) % 100 == 0:\n","        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n","                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch [1/100], Step [1/12], Loss: 0.9715\n","Epoch [1/100], Step [2/12], Loss: 0.7844\n","Epoch [1/100], Step [3/12], Loss: 0.6394\n","Epoch [1/100], Step [4/12], Loss: 0.6834\n","Epoch [1/100], Step [5/12], Loss: 0.4166\n","Epoch [1/100], Step [6/12], Loss: 0.4544\n","Epoch [1/100], Step [7/12], Loss: 0.9091\n","Epoch [1/100], Step [8/12], Loss: 0.7093\n","Epoch [1/100], Step [9/12], Loss: 0.8139\n","Epoch [1/100], Step [10/12], Loss: 0.8059\n","Epoch [1/100], Step [11/12], Loss: 0.7959\n","Epoch [1/100], Step [12/12], Loss: 0.3002\n","Epoch [2/100], Step [1/12], Loss: 0.7733\n","Epoch [2/100], Step [2/12], Loss: 0.5200\n","Epoch [2/100], Step [3/12], Loss: 0.4285\n","Epoch [2/100], Step [4/12], Loss: 0.6493\n","Epoch [2/100], Step [5/12], Loss: 0.3370\n","Epoch [2/100], Step [6/12], Loss: 0.4312\n","Epoch [2/100], Step [7/12], Loss: 0.8026\n","Epoch [2/100], Step [8/12], Loss: 0.6258\n","Epoch [2/100], Step [9/12], Loss: 0.7216\n","Epoch [2/100], Step [10/12], Loss: 0.7331\n","Epoch [2/100], Step [11/12], Loss: 0.7125\n","Epoch [2/100], Step [12/12], Loss: 0.3453\n","Epoch [3/100], Step [1/12], Loss: 0.7063\n","Epoch [3/100], Step [2/12], Loss: 0.5152\n","Epoch [3/100], Step [3/12], Loss: 0.4311\n","Epoch [3/100], Step [4/12], Loss: 0.6496\n","Epoch [3/100], Step [5/12], Loss: 0.3046\n","Epoch [3/100], Step [6/12], Loss: 0.3924\n","Epoch [3/100], Step [7/12], Loss: 0.7883\n","Epoch [3/100], Step [8/12], Loss: 0.5967\n","Epoch [3/100], Step [9/12], Loss: 0.7405\n","Epoch [3/100], Step [10/12], Loss: 0.7543\n","Epoch [3/100], Step [11/12], Loss: 0.7098\n","Epoch [3/100], Step [12/12], Loss: 0.3041\n","Epoch [4/100], Step [1/12], Loss: 0.6870\n","Epoch [4/100], Step [2/12], Loss: 0.4973\n","Epoch [4/100], Step [3/12], Loss: 0.4150\n","Epoch [4/100], Step [4/12], Loss: 0.6470\n","Epoch [4/100], Step [5/12], Loss: 0.2976\n","Epoch [4/100], Step [6/12], Loss: 0.3837\n","Epoch [4/100], Step [7/12], Loss: 0.7401\n","Epoch [4/100], Step [8/12], Loss: 0.5563\n","Epoch [4/100], Step [9/12], Loss: 0.7176\n","Epoch [4/100], Step [10/12], Loss: 0.7347\n","Epoch [4/100], Step [11/12], Loss: 0.6796\n","Epoch [4/100], Step [12/12], Loss: 0.3102\n","Epoch [5/100], Step [1/12], Loss: 0.6502\n","Epoch [5/100], Step [2/12], Loss: 0.4782\n","Epoch [5/100], Step [3/12], Loss: 0.4064\n","Epoch [5/100], Step [4/12], Loss: 0.6435\n","Epoch [5/100], Step [5/12], Loss: 0.2764\n","Epoch [5/100], Step [6/12], Loss: 0.3555\n","Epoch [5/100], Step [7/12], Loss: 0.7326\n","Epoch [5/100], Step [8/12], Loss: 0.5304\n","Epoch [5/100], Step [9/12], Loss: 0.7229\n","Epoch [5/100], Step [10/12], Loss: 0.7211\n","Epoch [5/100], Step [11/12], Loss: 0.6524\n","Epoch [5/100], Step [12/12], Loss: 0.3113\n","Epoch [6/100], Step [1/12], Loss: 0.6120\n","Epoch [6/100], Step [2/12], Loss: 0.4564\n","Epoch [6/100], Step [3/12], Loss: 0.4133\n","Epoch [6/100], Step [4/12], Loss: 0.6305\n","Epoch [6/100], Step [5/12], Loss: 0.2765\n","Epoch [6/100], Step [6/12], Loss: 0.3364\n","Epoch [6/100], Step [7/12], Loss: 0.7058\n","Epoch [6/100], Step [8/12], Loss: 0.4967\n","Epoch [6/100], Step [9/12], Loss: 0.7158\n","Epoch [6/100], Step [10/12], Loss: 0.7078\n","Epoch [6/100], Step [11/12], Loss: 0.6245\n","Epoch [6/100], Step [12/12], Loss: 0.3112\n","Epoch [7/100], Step [1/12], Loss: 0.5747\n","Epoch [7/100], Step [2/12], Loss: 0.4353\n","Epoch [7/100], Step [3/12], Loss: 0.4096\n","Epoch [7/100], Step [4/12], Loss: 0.6185\n","Epoch [7/100], Step [5/12], Loss: 0.2667\n","Epoch [7/100], Step [6/12], Loss: 0.3063\n","Epoch [7/100], Step [7/12], Loss: 0.6838\n","Epoch [7/100], Step [8/12], Loss: 0.4568\n","Epoch [7/100], Step [9/12], Loss: 0.7032\n","Epoch [7/100], Step [10/12], Loss: 0.6944\n","Epoch [7/100], Step [11/12], Loss: 0.5937\n","Epoch [7/100], Step [12/12], Loss: 0.3210\n","Epoch [8/100], Step [1/12], Loss: 0.5334\n","Epoch [8/100], Step [2/12], Loss: 0.4129\n","Epoch [8/100], Step [3/12], Loss: 0.4106\n","Epoch [8/100], Step [4/12], Loss: 0.6040\n","Epoch [8/100], Step [5/12], Loss: 0.2540\n","Epoch [8/100], Step [6/12], Loss: 0.2829\n","Epoch [8/100], Step [7/12], Loss: 0.6690\n","Epoch [8/100], Step [8/12], Loss: 0.4278\n","Epoch [8/100], Step [9/12], Loss: 0.6855\n","Epoch [8/100], Step [10/12], Loss: 0.6848\n","Epoch [8/100], Step [11/12], Loss: 0.5623\n","Epoch [8/100], Step [12/12], Loss: 0.3300\n","Epoch [9/100], Step [1/12], Loss: 0.4987\n","Epoch [9/100], Step [2/12], Loss: 0.3972\n","Epoch [9/100], Step [3/12], Loss: 0.4078\n","Epoch [9/100], Step [4/12], Loss: 0.5843\n","Epoch [9/100], Step [5/12], Loss: 0.2507\n","Epoch [9/100], Step [6/12], Loss: 0.2652\n","Epoch [9/100], Step [7/12], Loss: 0.6536\n","Epoch [9/100], Step [8/12], Loss: 0.3985\n","Epoch [9/100], Step [9/12], Loss: 0.6614\n","Epoch [9/100], Step [10/12], Loss: 0.6640\n","Epoch [9/100], Step [11/12], Loss: 0.5278\n","Epoch [9/100], Step [12/12], Loss: 0.3327\n","Epoch [10/100], Step [1/12], Loss: 0.4590\n","Epoch [10/100], Step [2/12], Loss: 0.3707\n","Epoch [10/100], Step [3/12], Loss: 0.4060\n","Epoch [10/100], Step [4/12], Loss: 0.5769\n","Epoch [10/100], Step [5/12], Loss: 0.2395\n","Epoch [10/100], Step [6/12], Loss: 0.2448\n","Epoch [10/100], Step [7/12], Loss: 0.6447\n","Epoch [10/100], Step [8/12], Loss: 0.3697\n","Epoch [10/100], Step [9/12], Loss: 0.6425\n","Epoch [10/100], Step [10/12], Loss: 0.6463\n","Epoch [10/100], Step [11/12], Loss: 0.4944\n","Epoch [10/100], Step [12/12], Loss: 0.3291\n","Epoch [11/100], Step [1/12], Loss: 0.4201\n","Epoch [11/100], Step [2/12], Loss: 0.3526\n","Epoch [11/100], Step [3/12], Loss: 0.4019\n","Epoch [11/100], Step [4/12], Loss: 0.5584\n","Epoch [11/100], Step [5/12], Loss: 0.2338\n","Epoch [11/100], Step [6/12], Loss: 0.2292\n","Epoch [11/100], Step [7/12], Loss: 0.6206\n","Epoch [11/100], Step [8/12], Loss: 0.3359\n","Epoch [11/100], Step [9/12], Loss: 0.6080\n","Epoch [11/100], Step [10/12], Loss: 0.6253\n","Epoch [11/100], Step [11/12], Loss: 0.4572\n","Epoch [11/100], Step [12/12], Loss: 0.3168\n","Epoch [12/100], Step [1/12], Loss: 0.3838\n","Epoch [12/100], Step [2/12], Loss: 0.3324\n","Epoch [12/100], Step [3/12], Loss: 0.4001\n","Epoch [12/100], Step [4/12], Loss: 0.5448\n","Epoch [12/100], Step [5/12], Loss: 0.2223\n","Epoch [12/100], Step [6/12], Loss: 0.2151\n","Epoch [12/100], Step [7/12], Loss: 0.5970\n","Epoch [12/100], Step [8/12], Loss: 0.2995\n","Epoch [12/100], Step [9/12], Loss: 0.5739\n","Epoch [12/100], Step [10/12], Loss: 0.6016\n","Epoch [12/100], Step [11/12], Loss: 0.4160\n","Epoch [12/100], Step [12/12], Loss: 0.3029\n","Epoch [13/100], Step [1/12], Loss: 0.3399\n","Epoch [13/100], Step [2/12], Loss: 0.3090\n","Epoch [13/100], Step [3/12], Loss: 0.3895\n","Epoch [13/100], Step [4/12], Loss: 0.5336\n","Epoch [13/100], Step [5/12], Loss: 0.2109\n","Epoch [13/100], Step [6/12], Loss: 0.1985\n","Epoch [13/100], Step [7/12], Loss: 0.5821\n","Epoch [13/100], Step [8/12], Loss: 0.2795\n","Epoch [13/100], Step [9/12], Loss: 0.5424\n","Epoch [13/100], Step [10/12], Loss: 0.5670\n","Epoch [13/100], Step [11/12], Loss: 0.3794\n","Epoch [13/100], Step [12/12], Loss: 0.2849\n","Epoch [14/100], Step [1/12], Loss: 0.3106\n","Epoch [14/100], Step [2/12], Loss: 0.2838\n","Epoch [14/100], Step [3/12], Loss: 0.3855\n","Epoch [14/100], Step [4/12], Loss: 0.5108\n","Epoch [14/100], Step [5/12], Loss: 0.2038\n","Epoch [14/100], Step [6/12], Loss: 0.1864\n","Epoch [14/100], Step [7/12], Loss: 0.5730\n","Epoch [14/100], Step [8/12], Loss: 0.2525\n","Epoch [14/100], Step [9/12], Loss: 0.5023\n","Epoch [14/100], Step [10/12], Loss: 0.5341\n","Epoch [14/100], Step [11/12], Loss: 0.3454\n","Epoch [14/100], Step [12/12], Loss: 0.2736\n","Epoch [15/100], Step [1/12], Loss: 0.2754\n","Epoch [15/100], Step [2/12], Loss: 0.2611\n","Epoch [15/100], Step [3/12], Loss: 0.3563\n","Epoch [15/100], Step [4/12], Loss: 0.4813\n","Epoch [15/100], Step [5/12], Loss: 0.1953\n","Epoch [15/100], Step [6/12], Loss: 0.1739\n","Epoch [15/100], Step [7/12], Loss: 0.5575\n","Epoch [15/100], Step [8/12], Loss: 0.2344\n","Epoch [15/100], Step [9/12], Loss: 0.4645\n","Epoch [15/100], Step [10/12], Loss: 0.4946\n","Epoch [15/100], Step [11/12], Loss: 0.3145\n","Epoch [15/100], Step [12/12], Loss: 0.2620\n","Epoch [16/100], Step [1/12], Loss: 0.2400\n","Epoch [16/100], Step [2/12], Loss: 0.2386\n","Epoch [16/100], Step [3/12], Loss: 0.3465\n","Epoch [16/100], Step [4/12], Loss: 0.4385\n","Epoch [16/100], Step [5/12], Loss: 0.1924\n","Epoch [16/100], Step [6/12], Loss: 0.1721\n","Epoch [16/100], Step [7/12], Loss: 0.5352\n","Epoch [16/100], Step [8/12], Loss: 0.2147\n","Epoch [16/100], Step [9/12], Loss: 0.4235\n","Epoch [16/100], Step [10/12], Loss: 0.4650\n","Epoch [16/100], Step [11/12], Loss: 0.2853\n","Epoch [16/100], Step [12/12], Loss: 0.2389\n","Epoch [17/100], Step [1/12], Loss: 0.2085\n","Epoch [17/100], Step [2/12], Loss: 0.2127\n","Epoch [17/100], Step [3/12], Loss: 0.3224\n","Epoch [17/100], Step [4/12], Loss: 0.4176\n","Epoch [17/100], Step [5/12], Loss: 0.1863\n","Epoch [17/100], Step [6/12], Loss: 0.1596\n","Epoch [17/100], Step [7/12], Loss: 0.4953\n","Epoch [17/100], Step [8/12], Loss: 0.1926\n","Epoch [17/100], Step [9/12], Loss: 0.3904\n","Epoch [17/100], Step [10/12], Loss: 0.4302\n","Epoch [17/100], Step [11/12], Loss: 0.2634\n","Epoch [17/100], Step [12/12], Loss: 0.2203\n","Epoch [18/100], Step [1/12], Loss: 0.1746\n","Epoch [18/100], Step [2/12], Loss: 0.1868\n","Epoch [18/100], Step [3/12], Loss: 0.3073\n","Epoch [18/100], Step [4/12], Loss: 0.3741\n","Epoch [18/100], Step [5/12], Loss: 0.1897\n","Epoch [18/100], Step [6/12], Loss: 0.1491\n","Epoch [18/100], Step [7/12], Loss: 0.4660\n","Epoch [18/100], Step [8/12], Loss: 0.1686\n","Epoch [18/100], Step [9/12], Loss: 0.3526\n","Epoch [18/100], Step [10/12], Loss: 0.3990\n","Epoch [18/100], Step [11/12], Loss: 0.2403\n","Epoch [18/100], Step [12/12], Loss: 0.1967\n","Epoch [19/100], Step [1/12], Loss: 0.1469\n","Epoch [19/100], Step [2/12], Loss: 0.1747\n","Epoch [19/100], Step [3/12], Loss: 0.2860\n","Epoch [19/100], Step [4/12], Loss: 0.3380\n","Epoch [19/100], Step [5/12], Loss: 0.1825\n","Epoch [19/100], Step [6/12], Loss: 0.1367\n","Epoch [19/100], Step [7/12], Loss: 0.4357\n","Epoch [19/100], Step [8/12], Loss: 0.1603\n","Epoch [19/100], Step [9/12], Loss: 0.3313\n","Epoch [19/100], Step [10/12], Loss: 0.3598\n","Epoch [19/100], Step [11/12], Loss: 0.2116\n","Epoch [19/100], Step [12/12], Loss: 0.1720\n","Epoch [20/100], Step [1/12], Loss: 0.1243\n","Epoch [20/100], Step [2/12], Loss: 0.1449\n","Epoch [20/100], Step [3/12], Loss: 0.2761\n","Epoch [20/100], Step [4/12], Loss: 0.3041\n","Epoch [20/100], Step [5/12], Loss: 0.1794\n","Epoch [20/100], Step [6/12], Loss: 0.1199\n","Epoch [20/100], Step [7/12], Loss: 0.3951\n","Epoch [20/100], Step [8/12], Loss: 0.1366\n","Epoch [20/100], Step [9/12], Loss: 0.3059\n","Epoch [20/100], Step [10/12], Loss: 0.3384\n","Epoch [20/100], Step [11/12], Loss: 0.2001\n","Epoch [20/100], Step [12/12], Loss: 0.1633\n","Epoch [21/100], Step [1/12], Loss: 0.1069\n","Epoch [21/100], Step [2/12], Loss: 0.1362\n","Epoch [21/100], Step [3/12], Loss: 0.2612\n","Epoch [21/100], Step [4/12], Loss: 0.2542\n","Epoch [21/100], Step [5/12], Loss: 0.1801\n","Epoch [21/100], Step [6/12], Loss: 0.1166\n","Epoch [21/100], Step [7/12], Loss: 0.3661\n","Epoch [21/100], Step [8/12], Loss: 0.1292\n","Epoch [21/100], Step [9/12], Loss: 0.2882\n","Epoch [21/100], Step [10/12], Loss: 0.2879\n","Epoch [21/100], Step [11/12], Loss: 0.1689\n","Epoch [21/100], Step [12/12], Loss: 0.1448\n","Epoch [22/100], Step [1/12], Loss: 0.0874\n","Epoch [22/100], Step [2/12], Loss: 0.1085\n","Epoch [22/100], Step [3/12], Loss: 0.2460\n","Epoch [22/100], Step [4/12], Loss: 0.2368\n","Epoch [22/100], Step [5/12], Loss: 0.1738\n","Epoch [22/100], Step [6/12], Loss: 0.0915\n","Epoch [22/100], Step [7/12], Loss: 0.3243\n","Epoch [22/100], Step [8/12], Loss: 0.1184\n","Epoch [22/100], Step [9/12], Loss: 0.2637\n","Epoch [22/100], Step [10/12], Loss: 0.2766\n","Epoch [22/100], Step [11/12], Loss: 0.1639\n","Epoch [22/100], Step [12/12], Loss: 0.1336\n","Epoch [23/100], Step [1/12], Loss: 0.0836\n","Epoch [23/100], Step [2/12], Loss: 0.1097\n","Epoch [23/100], Step [3/12], Loss: 0.2180\n","Epoch [23/100], Step [4/12], Loss: 0.1929\n","Epoch [23/100], Step [5/12], Loss: 0.1680\n","Epoch [23/100], Step [6/12], Loss: 0.0913\n","Epoch [23/100], Step [7/12], Loss: 0.2991\n","Epoch [23/100], Step [8/12], Loss: 0.1076\n","Epoch [23/100], Step [9/12], Loss: 0.2365\n","Epoch [23/100], Step [10/12], Loss: 0.2364\n","Epoch [23/100], Step [11/12], Loss: 0.1371\n","Epoch [23/100], Step [12/12], Loss: 0.1230\n","Epoch [24/100], Step [1/12], Loss: 0.0707\n","Epoch [24/100], Step [2/12], Loss: 0.0844\n","Epoch [24/100], Step [3/12], Loss: 0.2153\n","Epoch [24/100], Step [4/12], Loss: 0.1730\n","Epoch [24/100], Step [5/12], Loss: 0.1580\n","Epoch [24/100], Step [6/12], Loss: 0.0718\n","Epoch [24/100], Step [7/12], Loss: 0.2609\n","Epoch [24/100], Step [8/12], Loss: 0.0997\n","Epoch [24/100], Step [9/12], Loss: 0.2053\n","Epoch [24/100], Step [10/12], Loss: 0.2051\n","Epoch [24/100], Step [11/12], Loss: 0.1208\n","Epoch [24/100], Step [12/12], Loss: 0.1108\n","Epoch [25/100], Step [1/12], Loss: 0.0684\n","Epoch [25/100], Step [2/12], Loss: 0.0777\n","Epoch [25/100], Step [3/12], Loss: 0.1854\n","Epoch [25/100], Step [4/12], Loss: 0.1394\n","Epoch [25/100], Step [5/12], Loss: 0.1498\n","Epoch [25/100], Step [6/12], Loss: 0.0695\n","Epoch [25/100], Step [7/12], Loss: 0.2424\n","Epoch [25/100], Step [8/12], Loss: 0.0956\n","Epoch [25/100], Step [9/12], Loss: 0.1886\n","Epoch [25/100], Step [10/12], Loss: 0.1797\n","Epoch [25/100], Step [11/12], Loss: 0.1164\n","Epoch [25/100], Step [12/12], Loss: 0.1009\n","Epoch [26/100], Step [1/12], Loss: 0.0573\n","Epoch [26/100], Step [2/12], Loss: 0.0616\n","Epoch [26/100], Step [3/12], Loss: 0.1825\n","Epoch [26/100], Step [4/12], Loss: 0.1271\n","Epoch [26/100], Step [5/12], Loss: 0.1417\n","Epoch [26/100], Step [6/12], Loss: 0.0554\n","Epoch [26/100], Step [7/12], Loss: 0.1970\n","Epoch [26/100], Step [8/12], Loss: 0.0805\n","Epoch [26/100], Step [9/12], Loss: 0.1612\n","Epoch [26/100], Step [10/12], Loss: 0.1658\n","Epoch [26/100], Step [11/12], Loss: 0.0956\n","Epoch [26/100], Step [12/12], Loss: 0.0955\n","Epoch [27/100], Step [1/12], Loss: 0.0478\n","Epoch [27/100], Step [2/12], Loss: 0.0484\n","Epoch [27/100], Step [3/12], Loss: 0.1612\n","Epoch [27/100], Step [4/12], Loss: 0.0986\n","Epoch [27/100], Step [5/12], Loss: 0.1368\n","Epoch [27/100], Step [6/12], Loss: 0.0512\n","Epoch [27/100], Step [7/12], Loss: 0.1744\n","Epoch [27/100], Step [8/12], Loss: 0.0764\n","Epoch [27/100], Step [9/12], Loss: 0.1478\n","Epoch [27/100], Step [10/12], Loss: 0.1530\n","Epoch [27/100], Step [11/12], Loss: 0.0848\n","Epoch [27/100], Step [12/12], Loss: 0.0873\n","Epoch [28/100], Step [1/12], Loss: 0.0450\n","Epoch [28/100], Step [2/12], Loss: 0.0442\n","Epoch [28/100], Step [3/12], Loss: 0.1569\n","Epoch [28/100], Step [4/12], Loss: 0.0873\n","Epoch [28/100], Step [5/12], Loss: 0.1293\n","Epoch [28/100], Step [6/12], Loss: 0.0467\n","Epoch [28/100], Step [7/12], Loss: 0.1582\n","Epoch [28/100], Step [8/12], Loss: 0.0695\n","Epoch [28/100], Step [9/12], Loss: 0.1310\n","Epoch [28/100], Step [10/12], Loss: 0.1337\n","Epoch [28/100], Step [11/12], Loss: 0.0805\n","Epoch [28/100], Step [12/12], Loss: 0.0843\n","Epoch [29/100], Step [1/12], Loss: 0.0344\n","Epoch [29/100], Step [2/12], Loss: 0.0415\n","Epoch [29/100], Step [3/12], Loss: 0.1436\n","Epoch [29/100], Step [4/12], Loss: 0.0749\n","Epoch [29/100], Step [5/12], Loss: 0.1206\n","Epoch [29/100], Step [6/12], Loss: 0.0458\n","Epoch [29/100], Step [7/12], Loss: 0.1406\n","Epoch [29/100], Step [8/12], Loss: 0.0650\n","Epoch [29/100], Step [9/12], Loss: 0.1050\n","Epoch [29/100], Step [10/12], Loss: 0.1103\n","Epoch [29/100], Step [11/12], Loss: 0.0650\n","Epoch [29/100], Step [12/12], Loss: 0.0778\n","Epoch [30/100], Step [1/12], Loss: 0.0304\n","Epoch [30/100], Step [2/12], Loss: 0.0372\n","Epoch [30/100], Step [3/12], Loss: 0.1366\n","Epoch [30/100], Step [4/12], Loss: 0.0694\n","Epoch [30/100], Step [5/12], Loss: 0.1121\n","Epoch [30/100], Step [6/12], Loss: 0.0415\n","Epoch [30/100], Step [7/12], Loss: 0.1214\n","Epoch [30/100], Step [8/12], Loss: 0.0552\n","Epoch [30/100], Step [9/12], Loss: 0.0964\n","Epoch [30/100], Step [10/12], Loss: 0.0958\n","Epoch [30/100], Step [11/12], Loss: 0.0606\n","Epoch [30/100], Step [12/12], Loss: 0.0657\n","Epoch [31/100], Step [1/12], Loss: 0.0304\n","Epoch [31/100], Step [2/12], Loss: 0.0368\n","Epoch [31/100], Step [3/12], Loss: 0.1169\n","Epoch [31/100], Step [4/12], Loss: 0.0589\n","Epoch [31/100], Step [5/12], Loss: 0.0993\n","Epoch [31/100], Step [6/12], Loss: 0.0358\n","Epoch [31/100], Step [7/12], Loss: 0.1004\n","Epoch [31/100], Step [8/12], Loss: 0.0516\n","Epoch [31/100], Step [9/12], Loss: 0.0985\n","Epoch [31/100], Step [10/12], Loss: 0.0829\n","Epoch [31/100], Step [11/12], Loss: 0.0612\n","Epoch [31/100], Step [12/12], Loss: 0.0567\n","Epoch [32/100], Step [1/12], Loss: 0.0286\n","Epoch [32/100], Step [2/12], Loss: 0.0301\n","Epoch [32/100], Step [3/12], Loss: 0.1120\n","Epoch [32/100], Step [4/12], Loss: 0.0536\n","Epoch [32/100], Step [5/12], Loss: 0.0941\n","Epoch [32/100], Step [6/12], Loss: 0.0298\n","Epoch [32/100], Step [7/12], Loss: 0.0867\n","Epoch [32/100], Step [8/12], Loss: 0.0432\n","Epoch [32/100], Step [9/12], Loss: 0.0814\n","Epoch [32/100], Step [10/12], Loss: 0.0800\n","Epoch [32/100], Step [11/12], Loss: 0.0582\n","Epoch [32/100], Step [12/12], Loss: 0.0616\n","Epoch [33/100], Step [1/12], Loss: 0.0393\n","Epoch [33/100], Step [2/12], Loss: 0.0227\n","Epoch [33/100], Step [3/12], Loss: 0.1041\n","Epoch [33/100], Step [4/12], Loss: 0.0368\n","Epoch [33/100], Step [5/12], Loss: 0.0865\n","Epoch [33/100], Step [6/12], Loss: 0.0316\n","Epoch [33/100], Step [7/12], Loss: 0.0759\n","Epoch [33/100], Step [8/12], Loss: 0.0481\n","Epoch [33/100], Step [9/12], Loss: 0.0754\n","Epoch [33/100], Step [10/12], Loss: 0.0709\n","Epoch [33/100], Step [11/12], Loss: 0.0403\n","Epoch [33/100], Step [12/12], Loss: 0.0645\n","Epoch [34/100], Step [1/12], Loss: 0.0328\n","Epoch [34/100], Step [2/12], Loss: 0.0247\n","Epoch [34/100], Step [3/12], Loss: 0.1072\n","Epoch [34/100], Step [4/12], Loss: 0.0335\n","Epoch [34/100], Step [5/12], Loss: 0.0786\n","Epoch [34/100], Step [6/12], Loss: 0.0237\n","Epoch [34/100], Step [7/12], Loss: 0.0654\n","Epoch [34/100], Step [8/12], Loss: 0.0438\n","Epoch [34/100], Step [9/12], Loss: 0.0651\n","Epoch [34/100], Step [10/12], Loss: 0.0841\n","Epoch [34/100], Step [11/12], Loss: 0.0347\n","Epoch [34/100], Step [12/12], Loss: 0.0487\n","Epoch [35/100], Step [1/12], Loss: 0.0253\n","Epoch [35/100], Step [2/12], Loss: 0.0238\n","Epoch [35/100], Step [3/12], Loss: 0.0977\n","Epoch [35/100], Step [4/12], Loss: 0.0347\n","Epoch [35/100], Step [5/12], Loss: 0.0789\n","Epoch [35/100], Step [6/12], Loss: 0.0283\n","Epoch [35/100], Step [7/12], Loss: 0.0581\n","Epoch [35/100], Step [8/12], Loss: 0.0408\n","Epoch [35/100], Step [9/12], Loss: 0.0481\n","Epoch [35/100], Step [10/12], Loss: 0.0691\n","Epoch [35/100], Step [11/12], Loss: 0.0388\n","Epoch [35/100], Step [12/12], Loss: 0.0483\n","Epoch [36/100], Step [1/12], Loss: 0.0242\n","Epoch [36/100], Step [2/12], Loss: 0.0217\n","Epoch [36/100], Step [3/12], Loss: 0.1096\n","Epoch [36/100], Step [4/12], Loss: 0.0231\n","Epoch [36/100], Step [5/12], Loss: 0.0680\n","Epoch [36/100], Step [6/12], Loss: 0.0359\n","Epoch [36/100], Step [7/12], Loss: 0.0793\n","Epoch [36/100], Step [8/12], Loss: 0.0377\n","Epoch [36/100], Step [9/12], Loss: 0.0439\n","Epoch [36/100], Step [10/12], Loss: 0.0490\n","Epoch [36/100], Step [11/12], Loss: 0.0281\n","Epoch [36/100], Step [12/12], Loss: 0.0544\n","Epoch [37/100], Step [1/12], Loss: 0.0274\n","Epoch [37/100], Step [2/12], Loss: 0.0300\n","Epoch [37/100], Step [3/12], Loss: 0.0901\n","Epoch [37/100], Step [4/12], Loss: 0.0280\n","Epoch [37/100], Step [5/12], Loss: 0.0663\n","Epoch [37/100], Step [6/12], Loss: 0.0203\n","Epoch [37/100], Step [7/12], Loss: 0.0629\n","Epoch [37/100], Step [8/12], Loss: 0.0388\n","Epoch [37/100], Step [9/12], Loss: 0.0601\n","Epoch [37/100], Step [10/12], Loss: 0.0486\n","Epoch [37/100], Step [11/12], Loss: 0.0464\n","Epoch [37/100], Step [12/12], Loss: 0.0348\n","Epoch [38/100], Step [1/12], Loss: 0.0187\n","Epoch [38/100], Step [2/12], Loss: 0.0390\n","Epoch [38/100], Step [3/12], Loss: 0.0665\n","Epoch [38/100], Step [4/12], Loss: 0.0465\n","Epoch [38/100], Step [5/12], Loss: 0.0757\n","Epoch [38/100], Step [6/12], Loss: 0.0360\n","Epoch [38/100], Step [7/12], Loss: 0.0626\n","Epoch [38/100], Step [8/12], Loss: 0.0265\n","Epoch [38/100], Step [9/12], Loss: 0.0964\n","Epoch [38/100], Step [10/12], Loss: 0.0564\n","Epoch [38/100], Step [11/12], Loss: 0.0430\n","Epoch [38/100], Step [12/12], Loss: 0.0315\n","Epoch [39/100], Step [1/12], Loss: 0.0683\n","Epoch [39/100], Step [2/12], Loss: 0.0180\n","Epoch [39/100], Step [3/12], Loss: 0.0612\n","Epoch [39/100], Step [4/12], Loss: 0.0242\n","Epoch [39/100], Step [5/12], Loss: 0.0626\n","Epoch [39/100], Step [6/12], Loss: 0.0280\n","Epoch [39/100], Step [7/12], Loss: 0.0583\n","Epoch [39/100], Step [8/12], Loss: 0.0369\n","Epoch [39/100], Step [9/12], Loss: 0.0445\n","Epoch [39/100], Step [10/12], Loss: 0.0487\n","Epoch [39/100], Step [11/12], Loss: 0.0285\n","Epoch [39/100], Step [12/12], Loss: 0.0463\n","Epoch [40/100], Step [1/12], Loss: 0.0358\n","Epoch [40/100], Step [2/12], Loss: 0.0346\n","Epoch [40/100], Step [3/12], Loss: 0.0943\n","Epoch [40/100], Step [4/12], Loss: 0.0196\n","Epoch [40/100], Step [5/12], Loss: 0.0513\n","Epoch [40/100], Step [6/12], Loss: 0.0161\n","Epoch [40/100], Step [7/12], Loss: 0.0460\n","Epoch [40/100], Step [8/12], Loss: 0.0387\n","Epoch [40/100], Step [9/12], Loss: 0.0433\n","Epoch [40/100], Step [10/12], Loss: 0.0621\n","Epoch [40/100], Step [11/12], Loss: 0.0272\n","Epoch [40/100], Step [12/12], Loss: 0.0449\n","Epoch [41/100], Step [1/12], Loss: 0.0179\n","Epoch [41/100], Step [2/12], Loss: 0.0190\n","Epoch [41/100], Step [3/12], Loss: 0.0615\n","Epoch [41/100], Step [4/12], Loss: 0.0151\n","Epoch [41/100], Step [5/12], Loss: 0.0570\n","Epoch [41/100], Step [6/12], Loss: 0.0324\n","Epoch [41/100], Step [7/12], Loss: 0.0425\n","Epoch [41/100], Step [8/12], Loss: 0.0273\n","Epoch [41/100], Step [9/12], Loss: 0.0333\n","Epoch [41/100], Step [10/12], Loss: 0.0326\n","Epoch [41/100], Step [11/12], Loss: 0.0191\n","Epoch [41/100], Step [12/12], Loss: 0.0331\n","Epoch [42/100], Step [1/12], Loss: 0.0215\n","Epoch [42/100], Step [2/12], Loss: 0.0326\n","Epoch [42/100], Step [3/12], Loss: 0.0614\n","Epoch [42/100], Step [4/12], Loss: 0.0194\n","Epoch [42/100], Step [5/12], Loss: 0.0382\n","Epoch [42/100], Step [6/12], Loss: 0.0123\n","Epoch [42/100], Step [7/12], Loss: 0.0374\n","Epoch [42/100], Step [8/12], Loss: 0.0266\n","Epoch [42/100], Step [9/12], Loss: 0.0394\n","Epoch [42/100], Step [10/12], Loss: 0.0281\n","Epoch [42/100], Step [11/12], Loss: 0.0261\n","Epoch [42/100], Step [12/12], Loss: 0.0320\n","Epoch [43/100], Step [1/12], Loss: 0.0148\n","Epoch [43/100], Step [2/12], Loss: 0.0165\n","Epoch [43/100], Step [3/12], Loss: 0.0479\n","Epoch [43/100], Step [4/12], Loss: 0.0150\n","Epoch [43/100], Step [5/12], Loss: 0.0725\n","Epoch [43/100], Step [6/12], Loss: 0.0240\n","Epoch [43/100], Step [7/12], Loss: 0.0381\n","Epoch [43/100], Step [8/12], Loss: 0.0240\n","Epoch [43/100], Step [9/12], Loss: 0.0555\n","Epoch [43/100], Step [10/12], Loss: 0.0326\n","Epoch [43/100], Step [11/12], Loss: 0.0163\n","Epoch [43/100], Step [12/12], Loss: 0.0243\n","Epoch [44/100], Step [1/12], Loss: 0.0511\n","Epoch [44/100], Step [2/12], Loss: 0.0197\n","Epoch [44/100], Step [3/12], Loss: 0.0463\n","Epoch [44/100], Step [4/12], Loss: 0.0103\n","Epoch [44/100], Step [5/12], Loss: 0.0418\n","Epoch [44/100], Step [6/12], Loss: 0.0088\n","Epoch [44/100], Step [7/12], Loss: 0.0343\n","Epoch [44/100], Step [8/12], Loss: 0.0269\n","Epoch [44/100], Step [9/12], Loss: 0.0325\n","Epoch [44/100], Step [10/12], Loss: 0.0594\n","Epoch [44/100], Step [11/12], Loss: 0.0243\n","Epoch [44/100], Step [12/12], Loss: 0.0258\n","Epoch [45/100], Step [1/12], Loss: 0.0233\n","Epoch [45/100], Step [2/12], Loss: 0.0293\n","Epoch [45/100], Step [3/12], Loss: 0.0621\n","Epoch [45/100], Step [4/12], Loss: 0.0148\n","Epoch [45/100], Step [5/12], Loss: 0.0423\n","Epoch [45/100], Step [6/12], Loss: 0.0195\n","Epoch [45/100], Step [7/12], Loss: 0.0324\n","Epoch [45/100], Step [8/12], Loss: 0.0200\n","Epoch [45/100], Step [9/12], Loss: 0.0210\n","Epoch [45/100], Step [10/12], Loss: 0.0203\n","Epoch [45/100], Step [11/12], Loss: 0.0174\n","Epoch [45/100], Step [12/12], Loss: 0.0387\n","Epoch [46/100], Step [1/12], Loss: 0.0209\n","Epoch [46/100], Step [2/12], Loss: 0.0268\n","Epoch [46/100], Step [3/12], Loss: 0.0429\n","Epoch [46/100], Step [4/12], Loss: 0.0194\n","Epoch [46/100], Step [5/12], Loss: 0.0357\n","Epoch [46/100], Step [6/12], Loss: 0.0134\n","Epoch [46/100], Step [7/12], Loss: 0.0290\n","Epoch [46/100], Step [8/12], Loss: 0.0286\n","Epoch [46/100], Step [9/12], Loss: 0.0505\n","Epoch [46/100], Step [10/12], Loss: 0.0206\n","Epoch [46/100], Step [11/12], Loss: 0.0160\n","Epoch [46/100], Step [12/12], Loss: 0.0191\n"],"name":"stdout"}]},{"metadata":{"id":"A_EoMcLtgGkK","colab_type":"text"},"cell_type":"markdown","source":["# Test model"]},{"metadata":{"id":"iS2I2oxbgFzq","colab_type":"code","outputId":"d1ac50a5-ee46-4488-be59-3b273572cb6f","executionInfo":{"status":"ok","timestamp":1554767301134,"user_tz":240,"elapsed":283,"user":{"displayName":"Nick Kroeger","photoUrl":"","userId":"17654142378335071446"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["correct = 0\n","testSize = inputTest.shape[0]\n","predictions = []\n","\n","for i in range(testSize):\n","    #import pdb; pdb.set_trace()\n","    tempInputs = torch.FloatTensor(inputTest[i])\n","    output = net(Variable(tempInputs))\n","    if output.data[0] <= 0:\n","      predicted = -1.\n","    else:\n","      predicted = 1.\n","    predictions.append(predicted)\n","    correct += (predicted == labelTest[i][0])\n","\n","print('Accuracy of the network: %d %%' % (100 * correct / testSize))\n","\n","# Save the Model\n","#torch.save(net.state_dict(), 'model.pkl')\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Accuracy of the network: 80 %\n"],"name":"stdout"}]},{"metadata":{"id":"IE_054BKg2yu","colab_type":"text"},"cell_type":"markdown","source":["# Save the NN weights"]},{"metadata":{"id":"q0XDbM2fYZLL","colab_type":"code","colab":{}},"cell_type":"code","source":["# Save the model checkpoint\n","#torch.save(net.state_dict(), 'NN80PercentAccuracy.ckpt')\n","\n","import pickle\n","with open('NN80PercentTestAccuracy.ckpt', 'wb') as f:\n","    pickle.dump(net.state_dict(), f)\n","    "],"execution_count":0,"outputs":[]}]}